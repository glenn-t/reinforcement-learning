---
title: "Hyper-parameter experiments"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Setup: load libraries and function.

```{r}
library(ggplot2)
library(dplyr)
library(DT)

for(f in list.files("R", full.names = TRUE)) {
  source(f)
}

# Setup random agent
agent_random = new_agent_random()
# Get all states
all_states = get_all_states_and_winner()

# Set number of games per expiriment
N = 20000
# Set number of test games
N_test = 5000
# Set number of games to test the 'good' agent
N_train_p1 = 50000
# Helper function
winner_to_score = function(winner, symbol = 2) {
  winner[winner == 2] = -1
  if(symbol == 2) {
    winner = -winner
  }
  return(winner)
}
```

For testing, we will create a good player 1 agent.

```{r}
p1 = new_agent_01(all_states = all_states, symbol = 1, eps = 0, alpha = 0.5)
# p2 = new_agent_01(all_states = all_states, symbol = 2, eps = 10, alpha = 0.2)
# TODO increase later
out = train_agents(N_train_p1, p1, agent_random, print = FALSE)
p1 = out$p1
# p2 = out$p2
```

Check if the trained player 1 agent is any good.

```{r}
p1$eps = 0 # Don't explore
test_agents(N_test, p1, agent_random)
```

# Training against random agents

## Impact of epsilon

Try fixed epsilon values and for epsilon set to 1/N.

```{r}
# Create parameter grid
epsilon = c(1, 0, 0.001, 0.01, 0.1, 0.2, 0.5, 1)
epsilon_decay = rep(FALSE, length(epsilon))
epsilon_decay[1] = TRUE
```

```{r}
ls.metrics = as.list(rep(NA, length(epsilon)))
ls.score_history = as.list(rep(NA, length(epsilon)))

for(i in seq_along(epsilon)) {
  # Train agent
  p2 = new_agent_01(all_states = all_states, symbol = 2, eps = epsilon[i], eps_decay = epsilon_decay[i], alpha = 0.5)
  out = train_agents(N, agent_random, p2, print = FALSE)
  p2 = out$p2
  
  # Convert winner to score
  score = tibble(
    "iteration" = 1:N,
    "epsilon" = epsilon[i], 
    "epsilon_decay" = epsilon_decay[i],
    "score" = winner_to_score(out$winner)) %>%
    mutate(cummean = cummean(score))
  ls.score_history[[i]] = score
  
  ## Create key metrics
  metrics = tibble(
    "epsilon" = epsilon[i],
    "epsilon_decay" = epsilon_decay[i],
    "metric" = c("training", "random", "intelligent"),
    "score" = c(
      mean(score$score),
      test_agents(N_test, p1 = agent_random, p2 = p2, table = FALSE) %>%
        winner_to_score() %>%
        mean(),
      test_agents(1, p1 = p1, p2 = p2, table = FALSE) %>%
        winner_to_score() %>%
        mean()
    )
  )
  
  ls.metrics[[i]] = metrics
}
```

View performance metrics

```{r}
metrics = bind_rows(ls.metrics)
score_history = bind_rows(ls.score_history)
metrics %>% arrange(desc(score)) %>% DT::datatable()
```

```{r}
# Metric plot plot
ggplot(metrics, aes(x = epsilon, y = score, colour = metric)) +
  geom_line() + 
  geom_point()
```

```{r}
# Training plot
p = ggplot(score_history, aes(x = iteration, y = cummean, colour = paste0(epsilon, ifelse(epsilon_decay, "/N", "")))) +
  geom_line() + 
  labs(color = "epsilon")

p
p + scale_x_log10()
```

## Impact of alpha (learning rate)


```{r}
# Create parameter grid
alpha = c(1, 0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 1)
alpha_decay = c(TRUE, rep(FALSE, length(alpha)))
```

```{r}
ls.metrics = as.list(rep(NA, length(alpha)))
ls.score_history = as.list(rep(NA, length(alpha)))

for(i in seq_along(alpha)) {
  # Train agent
  p2 = new_agent_01(all_states = all_states, symbol = 2, alpha = alpha[i], alpha_decay = alpha_decay[i])
  out = train_agents(N, agent_random, p2, print = FALSE)
  p2 = out$p2
  
  # Convert winner to score
  score = tibble(
    "iteration" = 1:N,
    "alpha" = alpha[i], 
    "alpha_decay" = alpha_decay[i],
    "score" = winner_to_score(out$winner)) %>%
    mutate(cummean = cummean(score))
  ls.score_history[[i]] = score
  
  ## Create key metrics
  metrics = tibble(
    "alpha" = alpha[i],
    "alpha_decay" = alpha_decay[i],
    "metric" = c("training", "random", "intelligent"),
    "score" = c(
      mean(score$score),
      test_agents(N_test, p1 = agent_random, p2 = p2, table = FALSE) %>%
        winner_to_score() %>%
        mean(),
      test_agents(1, p1 = p1, p2 = p2, table = FALSE) %>%
        winner_to_score() %>%
        mean()
    )
  )
  
  ls.metrics[[i]] = metrics
}
```

View performance metrics

```{r}
metrics = bind_rows(ls.metrics)
score_history = bind_rows(ls.score_history)
metrics %>% arrange(desc(score)) %>% DT::datatable()
```

```{r}
# Metric plot plot
ggplot(metrics, aes(x = alpha, y = score, colour = metric)) +
  geom_line() + 
  geom_point()
```

```{r}
# Training plot
p = ggplot(score_history, aes(x = iteration, y = cummean, colour = paste0(alpha, ifelse(alpha_decay, "/N", "")))) +
  geom_line() + 
  labs(color = "alpha")

p
p + scale_x_log10()
```

# Conclusions:

  * Lower epsilon performs better (even zero). Zero had a variable performance though.
  * Epsilon decay helps, as you can have a higher decay at the start.
  * Alpha of 1 is terrible - no memory.
  * Most alpha values work ok for this duration of training. Low alpha values don't seem to perform as well. Also, alpha=1/N is learning too slowly.
  * Alpha decay - too much decay prevents learning after a high number of games. Need to tailor it to the number of games or try a different decay function.
  * There is the challenge of measuring performance. How do you make it objective and not dependent on the parameters of the opponent? Furthermore, if it does well against a bad opponent, what is that worth?

